# AI Tool Risk Inventory (Human-Centered Perspective)

## Why this exists
We find that lots of discussions about AI risk often focus on data.
But many people are uneasy about AI for a different reason:
the fear that systems without human context, ethics,
or common sense may eventually cross boundaries we did not intend to cross.

This small project looks at AI risk not only as a technical issue,
but as a human one—centered on judgment, accountability, and oversight.

---

## What this inventory tries to ask
Instead of asking only *what data is used*, I would ask the following:

- Does this tool assist humans, or quietly replace human decisions?
- Can a person easily question, override, or stop its output?
- Does the system read only patterns or understand context?
- When something is off, who is accountable?

---

## AI Tool Risk Inventory

| AI Tool | Human Judgment Required | Risk of Overreach | Accountability | Notes |
|-------|------------------------|------------------|---------------|------|
| ChatGPT (Public) | Low | High | Unclear | Outputs can appear authoritative without context |
| Copilot (Enterprise) | Medium | Medium | Organization | Governance exists, but user awareness is critical |
| Grammarly | Medium | Low–Medium | User | Subtle influence on tone and intent |
| Notion AI | Medium | Medium | Team / Organization | Shared environments amplify mistakes |
| Custom AI Tools | Depends | Potentially High | Organization / Developer | Risk depends on governance, not model choice |

---

## Key takeaway
AI risk is not only about data leakage.
Often, the deeper concern is the absence of human judgment,
ethical context, and clear responsibility.

Governance matters because it defines
where human responsibility begins—and where it must remain.

AI systems do not grow up or develop ethics the way humans do.
What we often call “AI ethics” is largely a set of constraints, guidelines, and feedback mechanisms designed by humans.
This makes human oversight and accountability essential, not optional.


---

## Scope
This is not a technical audit.
It is a lightweight, independent thinking exercise
intended to explore AI risk from a governance and human-centered perspective.
